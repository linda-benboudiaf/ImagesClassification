{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "data_dir = \"/home/lbenboudiaf/Bureau/ImagesClassification/dataset/GroceryStoreDataset/dataset/OneClasse\"\n",
    "model_name = \"squeezenet\"\n",
    "num_classes = 3\n",
    "batch_size = 8\n",
    "num_epochs = 200 #Valeur Initiale par défaut comme dans le TP du Prof \n",
    "feature_extract = True\n",
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
    "    since = time.time()\n",
    "    \n",
    "    val_acc_history = []\n",
    "    \n",
    "    loss_val = []\n",
    "    loss_train = []\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # récupéré le model et calculer sa fonction loss \n",
    "                    if is_inception and phase == 'train':\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase, Backward \n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "            \n",
    "            if phase == 'train':\n",
    "                loss_train.append(epoch_loss)\n",
    "            else:\n",
    "                loss_val.append(epoch_loss)\n",
    "                \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history, loss_train, loss_val\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Datasets and Dataloaders...\n",
      "Params to learn:\n",
      "\t classifier.1.weight\n",
      "\t classifier.1.bias\n"
     ]
    }
   ],
   "source": [
    "#Definition du model Squeezenet.\n",
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == \"squeezenet\":\n",
    "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model_ft.num_classes = num_classes\n",
    "        input_size = 224\n",
    "    else:\n",
    "        print(\"Invalid model name\")\n",
    "        exit()\n",
    "\n",
    "    return model_ft, input_size\n",
    "\n",
    "# Initialisé le model pour ce dataset \n",
    "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
    "import os\n",
    "# Augmentaion et normalisation de données pour le jeux d'apprentissage.  \n",
    "# Normalisation des jeux de test et de validation \n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(input_size),\n",
    "        transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "print(\"Initializing Datasets and Dataloaders...\")\n",
    "\n",
    "# Generation de jeux d'apprentissage et de validation \n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val', 'test']}\n",
    "# Creation des dataloaders pour jeux d'apprentissage et de validation. \n",
    "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val', 'test']}\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Envoyer le model au GPU\n",
    "model_ft = model_ft.to(device)\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "# Observation des paramètres sont optimisé \n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.0001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/199\n",
      "----------\n",
      "train Loss: 1.1106 Acc: 0.3041\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 1/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 2/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 3/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 4/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2905\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 5/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 6/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 7/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 8/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 9/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2838\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 10/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 11/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 12/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 13/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 14/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 15/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 16/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 17/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 18/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 19/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 20/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 21/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 22/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 23/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 24/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 25/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2905\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 26/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2838\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 27/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 28/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 29/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 30/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 31/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2838\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 32/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 33/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 34/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 35/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 36/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2838\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 37/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 38/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 39/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 40/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 41/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 42/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 43/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 44/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 45/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 46/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 47/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 48/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 49/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 50/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 51/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 52/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 53/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 54/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 55/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 56/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 57/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2838\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 58/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2905\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 59/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 60/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 61/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 62/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 63/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 64/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 65/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 66/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 67/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 68/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 69/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 70/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 71/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 72/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 73/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 74/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 75/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 76/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 77/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 78/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 79/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 80/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 81/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 82/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 83/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 84/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 85/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 86/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 87/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 88/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 89/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 90/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 91/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 92/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 93/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 94/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 95/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 96/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 97/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2905\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 98/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 99/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 100/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 101/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 102/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2905\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 103/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 104/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 105/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 106/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 107/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 108/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 109/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 110/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 111/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 112/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 113/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 114/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 115/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 116/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 117/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 118/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 119/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 120/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 121/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 122/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 123/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 124/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 125/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 126/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 127/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 128/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 129/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 130/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 131/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2905\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 132/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 133/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 134/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 135/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 136/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 137/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 138/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 139/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 140/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 141/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 142/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 143/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 144/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 145/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 146/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 147/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 148/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 149/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 150/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 151/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 152/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 153/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 154/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 155/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 156/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2905\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 157/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2838\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 158/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 159/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 160/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 161/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 162/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 163/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 164/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 165/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 166/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2838\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 167/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 168/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 169/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 170/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 171/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 172/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 173/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2838\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 174/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 175/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 176/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 177/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 178/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 179/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2905\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 180/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 181/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 182/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 183/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 184/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2905\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 185/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 186/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 187/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 188/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 189/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 190/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 191/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 192/199\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.0986 Acc: 0.2838\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 193/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 194/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 195/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2905\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 196/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 197/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2905\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 198/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Epoch 199/199\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.2872\n",
      "val Loss: 1.0986 Acc: 0.2872\n",
      "\n",
      "Training complete in 84m 58s\n",
      "Best val Acc: 0.287162\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "scratch_model,_ = initialize_model(model_name, num_classes, feature_extract=False, use_pretrained=False)\n",
    "scratch_model = scratch_model.to(device)\n",
    "scratch_optimizer = optim.SGD(scratch_model.parameters(), lr=0.001, momentum=0.9)\n",
    "scratch_criterion = nn.CrossEntropyLoss()\n",
    "_,scratch_hist, train_loss, val_loss = train_model(scratch_model, dataloaders_dict, scratch_criterion, scratch_optimizer, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))\n",
    "\n",
    "#ohist = []\n",
    "shist = []\n",
    "\n",
    "#ohist = [h.cpu().numpy() for h in hist]\n",
    "shist = [h.cpu().numpy() for h in scratch_hist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.1105721770106136, 1.0986261947734937, 1.0986292974368945, 1.0986000138360101, 1.098619493278297, 1.0986167224677834, 1.0986213039707493, 1.098639820073102, 1.0985933445595406, 1.0986179500012785, 1.0986154272749618, 1.0986210977708972, 1.0986234239629797, 1.098598119374868, 1.0986134812638566, 1.0986016312161007, 1.0986230373382568, 1.09863065706717, 1.0986128143362097, 1.0986134715982385, 1.0986157816809576, 1.0986086581204388, 1.0986047274357564, 1.0986008966291272, 1.0986319877005912, 1.0986141514133763, 1.098606531684463, 1.0986167289115287, 1.098626584620089, 1.0986141675227397, 1.0986123503865421, 1.0986078977584839, 1.0986154627155613, 1.098618829572523, 1.0986315978539956, 1.0986311822324186, 1.098633115356033, 1.0986208561304454, 1.0986000073922646, 1.0986145895880621, 1.0986179274481696, 1.0986030262869757, 1.0986183269603833, 1.098614280288284, 1.0986162649618614, 1.0986174602766294, 1.0985995434425972, 1.098614686244243, 1.098620202090289, 1.0986112452842094, 1.0986175408234466, 1.0986128884392816, 1.0986063254846108, 1.0986229986757845, 1.0986130591985341, 1.0986095956853918, 1.0986143576132286, 1.0986057391037811, 1.0986070794028204, 1.098614434938173, 1.0986150922002018, 1.0986182915197837, 1.0986091639544513, 1.098605774544381, 1.0986072114996008, 1.0986160104339187, 1.0986149601034216, 1.0986150857564565, 1.0986231565475464, 1.0986044374672141, 1.098609460366739, 1.0986005776637309, 1.0986133459452037, 1.0986173120704856, 1.0986124502645958, 1.098609531247938, 1.0986097245602995, 1.0986148054535325, 1.0986150954220746, 1.0986219096828151, 1.0986141675227397, 1.098617421614157, 1.0986111196311745, 1.098605065732389, 1.0986117188994948, 1.098616944776999, 1.0986029006339408, 1.0986053299259495, 1.0986219483452875, 1.0986092058387962, 1.0986151727470193, 1.098609302494977, 1.098599971951665, 1.0986212298676774, 1.098612041086764, 1.098615881559011, 1.0985948942803048, 1.0986169802175987, 1.0986190454379932, 1.098616757908383, 1.098607011743494, 1.098614737794206, 1.0986137325699266, 1.0986171864174508, 1.098616300402461, 1.0986172540767773, 1.0986265105170172, 1.0986163938367688, 1.0986121731835443, 1.0986151888563827, 1.0986045727858673, 1.0986191066535744, 1.098615830009048, 1.0986165259335492, 1.0986198251311843, 1.0986138904416882, 1.098611374159117, 1.0986161876369167, 1.098613258954641, 1.0986019372940063, 1.098606531684463, 1.0986176084827732, 1.0986179177825515, 1.09860313905252, 1.0986092541668866, 1.0986097374477901, 1.098622711929115, 1.0986124502645958, 1.098612456708341, 1.0986123085021973, 1.0986194256189707, 1.0986144156069368, 1.0986118477744025, 1.0986158557840295, 1.0986075465743605, 1.098614770012933, 1.098615362837508, 1.0986148602253683, 1.0986049980730623, 1.0986160426526457, 1.0986114385965708, 1.0986199604498375, 1.0986106105752893, 1.098612218289762, 1.0986201440965808, 1.0986091059607428, 1.0986081941707715, 1.0986173056267403, 1.0986176052609005, 1.0986143608351011, 1.098614808675405, 1.0986055586789105, 1.0986124534864683, 1.0986118928806201, 1.098610230394312, 1.098609676232209, 1.0986084293674778, 1.0986202053121619, 1.0986218516891066, 1.0986098469914616, 1.0986128852174089, 1.098612675795684, 1.0986159395527195, 1.0986013186944497, 1.0986258790299699, 1.098611873549384, 1.098606041959814, 1.0986103914879464, 1.0986178597888432, 1.0986144284944277, 1.0986135746981647, 1.098609064076398, 1.098612727345647, 1.0986126919050474, 1.0986058003193624, 1.0986055393476744, 1.0986118961024929, 1.0986141095290314, 1.0986197639156032, 1.0986132202921688, 1.0986128368893184, 1.098614882778477, 1.0986142094070848, 1.0986089609764718, 1.0986115223652608, 1.0986270066854116, 1.098606116062886, 1.098608522801786, 1.0986123085021973, 1.098614206185212, 1.0986179822200053, 1.0986204630619771, 1.0986151405282922, 1.0986143221726288, 1.098611084190575, 1.0986102046193302, 1.0986192097535004, 1.0986115674714785, 1.0986135328138196, 1.0986175988171551]\n",
      "[1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.title(\"Validation Loss vs. Training Loss, Learning rate : 10^-4\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.ylabel(\"Loss rate\")\n",
    "plt.plot(range(1,num_epochs+1),train_loss,label=\"Training Scratch\")\n",
    "plt.plot(range(1,num_epochs+1),val_loss,label=\"Validation\")\n",
    "plt.ylim((0,1.))\n",
    "plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
    "plt.legend()\n",
    "plt.rcParams[\"figure.figsize\"] = (10,6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
