train_loss = [1.1105721770106136, 1.0986261947734937, 1.0986292974368945, 1.0986000138360101, 1.098619493278297, 1.0986167224677834, 1.0986213039707493, 1.098639820073102, 1.0985933445595406, 1.0986179500012785, 1.0986154272749618, 1.0986210977708972, 1.0986234239629797, 1.098598119374868, 1.0986134812638566, 1.0986016312161007, 1.0986230373382568, 1.09863065706717, 1.0986128143362097, 1.0986134715982385, 1.0986157816809576, 1.0986086581204388, 1.0986047274357564, 1.0986008966291272, 1.0986319877005912, 1.0986141514133763, 1.098606531684463, 1.0986167289115287, 1.098626584620089, 1.0986141675227397, 1.0986123503865421, 1.0986078977584839, 1.0986154627155613, 1.098618829572523, 1.0986315978539956, 1.0986311822324186, 1.098633115356033, 1.0986208561304454, 1.0986000073922646, 1.0986145895880621, 1.0986179274481696, 1.0986030262869757, 1.0986183269603833, 1.098614280288284, 1.0986162649618614, 1.0986174602766294, 1.0985995434425972, 1.098614686244243, 1.098620202090289, 1.0986112452842094, 1.0986175408234466, 1.0986128884392816, 1.0986063254846108, 1.0986229986757845, 1.0986130591985341, 1.0986095956853918, 1.0986143576132286, 1.0986057391037811, 1.0986070794028204, 1.098614434938173, 1.0986150922002018, 1.0986182915197837, 1.0986091639544513, 1.098605774544381, 1.0986072114996008, 1.0986160104339187, 1.0986149601034216, 1.0986150857564565, 1.0986231565475464, 1.0986044374672141, 1.098609460366739, 1.0986005776637309, 1.0986133459452037, 1.0986173120704856, 1.0986124502645958, 1.098609531247938, 1.0986097245602995, 1.0986148054535325, 1.0986150954220746, 1.0986219096828151, 1.0986141675227397, 1.098617421614157, 1.0986111196311745, 1.098605065732389, 1.0986117188994948, 1.098616944776999, 1.0986029006339408, 1.0986053299259495, 1.0986219483452875, 1.0986092058387962, 1.0986151727470193, 1.098609302494977, 1.098599971951665, 1.0986212298676774, 1.098612041086764, 1.098615881559011, 1.0985948942803048, 1.0986169802175987, 1.0986190454379932, 1.098616757908383, 1.098607011743494, 1.098614737794206, 1.0986137325699266, 1.0986171864174508, 1.098616300402461, 1.0986172540767773, 1.0986265105170172, 1.0986163938367688, 1.0986121731835443, 1.0986151888563827, 1.0986045727858673, 1.0986191066535744, 1.098615830009048, 1.0986165259335492, 1.0986198251311843, 1.0986138904416882, 1.098611374159117, 1.0986161876369167, 1.098613258954641, 1.0986019372940063, 1.098606531684463, 1.0986176084827732, 1.0986179177825515, 1.09860313905252, 1.0986092541668866, 1.0986097374477901, 1.098622711929115, 1.0986124502645958, 1.098612456708341, 1.0986123085021973, 1.0986194256189707, 1.0986144156069368, 1.0986118477744025, 1.0986158557840295, 1.0986075465743605, 1.098614770012933, 1.098615362837508, 1.0986148602253683, 1.0986049980730623, 1.0986160426526457, 1.0986114385965708, 1.0986199604498375, 1.0986106105752893, 1.098612218289762, 1.0986201440965808, 1.0986091059607428, 1.0986081941707715, 1.0986173056267403, 1.0986176052609005, 1.0986143608351011, 1.098614808675405, 1.0986055586789105, 1.0986124534864683, 1.0986118928806201, 1.098610230394312, 1.098609676232209, 1.0986084293674778, 1.0986202053121619, 1.0986218516891066, 1.0986098469914616, 1.0986128852174089, 1.098612675795684, 1.0986159395527195, 1.0986013186944497, 1.0986258790299699, 1.098611873549384, 1.098606041959814, 1.0986103914879464, 1.0986178597888432, 1.0986144284944277, 1.0986135746981647, 1.098609064076398, 1.098612727345647, 1.0986126919050474, 1.0986058003193624, 1.0986055393476744, 1.0986118961024929, 1.0986141095290314, 1.0986197639156032, 1.0986132202921688, 1.0986128368893184, 1.098614882778477, 1.0986142094070848, 1.0986089609764718, 1.0986115223652608, 1.0986270066854116, 1.098606116062886, 1.098608522801786, 1.0986123085021973, 1.098614206185212, 1.0986179822200053, 1.0986204630619771, 1.0986151405282922, 1.0986143221726288, 1.098611084190575, 1.0986102046193302, 1.0986192097535004, 1.0986115674714785, 1.0986135328138196, 1.0986175988171551]

val_loss = [1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973, 1.0986123085021973]

num_epochs = 200
import matplotlib.pyplot as plt
import numpy as np
plt.title("Validation Loss vs. Training Loss, Learning rate : 10^-4")
plt.xlabel("Training Epochs")
plt.ylabel("Loss rate")
plt.plot(range(1,num_epochs+1),train_loss,label="Training Scratch")
plt.plot(range(1,num_epochs+1),val_loss,label="Validation")
plt.ylim((0,2.))
plt.xticks(np.arange(1, num_epochs+1, 1.0))
plt.legend()
plt.rcParams["figure.figsize"] = (10,6)
plt.show()
